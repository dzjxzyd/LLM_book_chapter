{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95NTckuFZZzm"
      },
      "source": [
        "### requirements for the following codings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UO71IBS6ZgZV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b6e79e8-454d-4c5a-98d4-fe3a0784c3cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fair-esm\n",
            "  Downloading fair_esm-2.0.0-py3-none-any.whl (93 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/93.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.2/93.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fair-esm\n",
            "Successfully installed fair-esm-2.0.0\n"
          ]
        }
      ],
      "source": [
        "### packages required\n",
        "!pip install fair-esm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import esm\n",
        "from keras.layers import Input, Dense, Activation, BatchNormalization, Flatten, Conv1D\n",
        "from keras.layers import Dropout, AveragePooling1D, MaxPooling1D\n",
        "from keras.models import Sequential,Model, load_model\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import keras\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "if tf.test.gpu_device_name():\n",
        "    print('GPU found')\n",
        "    tf.config.experimental.set_visible_devices(tf.config.list_physical_devices('GPU')[0], 'GPU') # set the deep learning with GPU\n",
        "else:\n",
        "    print(\"No GPU found\")"
      ],
      "metadata": {
        "id": "DON02fGXgd8v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22671a28-32de-4086-c8a9-b034465386e2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m91cA0H5w_eY"
      },
      "source": [
        "### peptide embeddings with esm2_t6_8M_UR50D pretrained models\n",
        "6 layers, 8M parameters, dataset: UR50/D 2021_04, embedding dimension: 320\n",
        "mode download URL: https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t6_8M_UR50D.pt\n",
        "\n",
        "original ESM github site is https://github.com/facebookresearch/esm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pl7XVx5HZsHf"
      },
      "outputs": [],
      "source": [
        "def esm_embeddings(esm2, esm2_alphabet, peptide_sequence_list):\n",
        "  # NOTICE: ESM for embeddings is quite RAM usage, if your sequence is too long,\n",
        "  #         or you have too many sequences for transformation in a single converting, (i personally suggest to do the embedding one by one if you are running locally or with T4 GPU at google colab)\n",
        "  #         you computer might automatically kill the job.\n",
        "  # load the python packages\n",
        "  import torch\n",
        "  import esm\n",
        "  import collections\n",
        "  import pandas as pd\n",
        "  import gc\n",
        "  # choose cuda for acceleration if available\n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "  else:\n",
        "    device = torch.device(\"cpu\")\n",
        "  esm2.eval()\n",
        "  esm2 = esm2.to(device)\n",
        "\n",
        "\n",
        "  batch_converter = esm2_alphabet.get_batch_converter() # this function is to tokenize your peptide sequences from amino acid sequence into numbers\n",
        "\n",
        "  # load the peptide sequence list into the bach_converter\n",
        "  batch_labels, batch_strs, batch_tokens = batch_converter(peptide_sequence_list)\n",
        "  batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
        "  ## batch tokens are the tokenized results of the whole data set\n",
        "  batch_tokens = batch_tokens.to(device)\n",
        "  # Extract per-residue representations (on CPU)\n",
        "  with torch.no_grad():\n",
        "      # Here we export the last layer of the EMS model output as the representation of the peptides\n",
        "      # model'esm2_t6_8M_UR50D' only has 6 layers, and therefore repr_layers parameters is equal to 6\n",
        "      # if you want to change the esm model, for example: esm2_t12_35M_UR50D, which has 12 layers, they you will need to change the 6 to 12 at the following two line\n",
        "      results = esm2(batch_tokens, repr_layers=[6], return_contacts=False)\n",
        "  token_representations = results[\"representations\"][6].cpu() # the representation is generated for each amino acid residue; for example, if you have 3 residues, then the ourput shape is 3 * 320, 3 is the number of residues and the 320 is the vector for a single rediue\n",
        "\n",
        "  # Generate per-sequence representations via averaging\n",
        "  # NOTE: token 0 is always a beginning-of-sequence token, so we will discard it and start from token 1\n",
        "  sequence_representations = []\n",
        "  for i, tokens_len in enumerate(batch_lens):\n",
        "      sequence_representations.append(token_representations[i, 1 : tokens_len - 1].mean(0))\n",
        "  # save dataset\n",
        "  # sequence_representations is a list and each element is a tensor\n",
        "  embeddings_results = collections.defaultdict(list)\n",
        "  for i in range(len(sequence_representations)):\n",
        "      # tensor format can be transformed as numpy format sequence_representations[0].numpy() or sequence_representations[0].to_list\n",
        "      each_seq_rep = sequence_representations[i].tolist()\n",
        "      for each_element in each_seq_rep:\n",
        "          embeddings_results[i].append(each_element)\n",
        "  embeddings_results = pd.DataFrame(embeddings_results).T\n",
        "  del  batch_labels, batch_strs, batch_tokens, results, token_representations # delete those variables to save GPU memory\n",
        "  gc.collect() # release the GPU memory\n",
        "  return embeddings_results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RddxugbsdR1Y"
      },
      "source": [
        "### data loading and embeddings\n",
        "assume you have already split your dataset as a train and a test dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# select the ESM model for embeddings (you can select you desired model from https://github.com/facebookresearch/esm)\n",
        "# NOTICE: if you choose other model, the following model architecture might not be very compitable\n",
        "#         bseides,please revise the correspdoning parameters in esm_embeddings function (layers for feature extraction)\n",
        "model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()"
      ],
      "metadata": {
        "id": "IDPlUoVUgmmd"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "LNlD8pvizH84"
      },
      "outputs": [],
      "source": [
        "# training dataset loading\n",
        "dataset = pd.read_excel('bitter_train.xlsx',header=0, index_col = None)\n",
        "# generate embedding for seqeunces\n",
        "sequence_list = dataset['sequence']\n",
        "embeddings_results = pd.DataFrame()\n",
        "for seq in sequence_list:\n",
        "    # the setting is just following the input format setting in ESM model, [name,sequence]\n",
        "    tuple_sequence = tuple([seq,seq])\n",
        "    peptide_sequence_list = []\n",
        "    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n",
        "    # employ ESM model for converting and save the converted data in csv format\n",
        "    one_seq_embeddings = esm_embeddings(model, alphabet, peptide_sequence_list)\n",
        "    embeddings_results= pd.concat([embeddings_results,one_seq_embeddings])\n",
        "embeddings_results.to_csv('train_main_esm2_t6_8M_UR50D_unified_320_dimension.csv')\n",
        "\n",
        "# test dataset loading\n",
        "dataset = pd.read_excel('bitter_test.xlsx',header=0, index_col = None)\n",
        "# generate embedding for seqeunces\n",
        "sequence_list = dataset['sequence']\n",
        "embeddings_results = pd.DataFrame()\n",
        "# embedding all the peptide one by one\n",
        "for seq in sequence_list:\n",
        "    # the setting is just following the input format setting in ESM model, [name,sequence]\n",
        "    tuple_sequence = tuple([seq,seq] )\n",
        "    peptide_sequence_list = []\n",
        "    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n",
        "    # employ ESM model for converting and save the converted data in csv format\n",
        "    one_seq_embeddings = esm_embeddings(model, alphabet, peptide_sequence_list)\n",
        "    embeddings_results= pd.concat([embeddings_results,one_seq_embeddings])\n",
        "embeddings_results.to_csv('test_main_esm2_t6_8M_UR50D_unified_320_dimension.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### dataset division\n",
        "ready for training and evaluation"
      ],
      "metadata": {
        "id": "j7rGiqVLwSH4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "Xk13-JbBXAph"
      },
      "outputs": [],
      "source": [
        "# training dataset loading\n",
        "dataset = pd.read_excel('bitter_train.xlsx',na_filter = False) # take care the NA sequence problem\n",
        "sequence_list = dataset['sequence']\n",
        "y_train = dataset['label']\n",
        "y_train = np.array(y_train) # transformed as np.array for CNN model\n",
        "\n",
        "# test dataset loading\n",
        "dataset = pd.read_excel('bitter_test.xlsx',na_filter = False) # take care the NA sequence problem\n",
        "sequence_list = dataset['sequence']\n",
        "y_test = dataset['label']\n",
        "y_test = np.array(y_test) # transformed as np.array for CNN model\n",
        "\n",
        "# loading the peptide embddings\n",
        "X_train = pd.read_csv('train_main_esm2_t6_8M_UR50D_unified_320_dimension.csv',header=0, index_col = 0,delimiter=',')\n",
        "X_test = pd.read_csv('test_main_esm2_t6_8M_UR50D_unified_320_dimension.csv',header=0, index_col = 0,delimiter=',')\n",
        "# generate the validation dataset for early stopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=123, shuffle= True, stratify =y_train )\n",
        "\n",
        "# transform the dataset into numpy format\n",
        "X_train = np.array(X_train)\n",
        "X_valid = np.array(X_valid)\n",
        "X_test = np.array(X_test)\n",
        "\n",
        "# normalize the X data range\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train) # normalize X to 0-1 range\n",
        "X_valid = scaler.transform(X_valid) # normalize X to 0-1 range\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "HubTATKXslKw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "419facb9-27ab-4e88-9001-40dcdb8f3f78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(409, 320)\n",
            "(103, 320)\n",
            "(128, 320)\n",
            "(409,)\n",
            "(103,)\n",
            "(128,)\n"
          ]
        }
      ],
      "source": [
        "# check the dimension of the dataset before model development\n",
        "print(X_train.shape)\n",
        "print(X_valid.shape)\n",
        "print(X_test.shape)\n",
        "\n",
        "print(y_train.shape)\n",
        "print(y_valid.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RGqyaijk-Kq"
      },
      "source": [
        "### dataset loading and embeddings\n",
        "(if you only have one dataset and need to split them)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# select the ESM model for embeddings (you can select you desired model from https://github.com/facebookresearch/esm)\n",
        "# NOTICE: if you choose other model, the following model architecture might not be very compitable\n",
        "#         bseides,please revise the correspdoning parameters in esm_embeddings function (layers for feature extraction)\n",
        "model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()"
      ],
      "metadata": {
        "id": "4jpuP4v4tDZN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "63DekwCSlGsh"
      },
      "outputs": [],
      "source": [
        "# whole dataset loading and dataset splitting\n",
        "dataset = pd.read_excel('bitter_single_dataset_example.xlsx',header=0, index_col = None)\n",
        "\n",
        "# generate the peptide embeddings\n",
        "sequence_list = dataset['sequence']\n",
        "embeddings_results = pd.DataFrame()\n",
        "for seq in sequence_list:\n",
        "    # the setting is just following the input format setting in ESM model, [name,sequence]\n",
        "    tuple_sequence = tuple([seq,seq])\n",
        "    peptide_sequence_list = []\n",
        "    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n",
        "    # employ ESM model for converting and save the converted data in csv format\n",
        "    one_seq_embeddings = esm_embeddings(model, alphabet, peptide_sequence_list)\n",
        "    embeddings_results= pd.concat([embeddings_results,one_seq_embeddings])\n",
        "embeddings_results.to_csv('whole_sample_dataset_esm2_t6_8M_UR50D_unified_320_dimension.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### dataset division\n",
        "ready for training and evaluation"
      ],
      "metadata": {
        "id": "yvuT95J2wXrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the y dataset for model development\n",
        "y = dataset['label']\n",
        "y = np.array(y) # transformed as np.array for CNN model\n",
        "\n",
        "# read the peptide embeddings\n",
        "X_data = pd.read_csv('whole_sample_dataset_esm2_t6_8M_UR50D_unified_320_dimension.csv',header=0, index_col = 0,delimiter=',')\n",
        "X = np.array(X_data)\n",
        "\n",
        "# split dataset as training and test dataset as ratio of 8:2\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=123, shuffle= True, stratify =y)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=234, shuffle= True, stratify =y_train)\n",
        "\n",
        "\n",
        "# normalize the X data range\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train) # normalize X to 0-1 range\n",
        "X_valid = scaler.transform(X_valid)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "NcAG9Pg3qneT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vijDuRdtldHJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c29ef156-b547-4e2a-f36a-a1b64ae86d04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(460, 320)\n",
            "(52, 320)\n",
            "(128, 320)\n",
            "(460,)\n",
            "(52,)\n",
            "(128,)\n"
          ]
        }
      ],
      "source": [
        "# check the dimension of the dataset before model development\n",
        "print(X_train.shape)\n",
        "print(X_valid.shape)\n",
        "print(X_test.shape)\n",
        "\n",
        "print(y_train.shape)\n",
        "print(y_valid.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3Fagh9Iw83q"
      },
      "source": [
        "### Model architecture"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ESM_CNN(X_train, y_train, X_test, y_test):\n",
        "  inputShape=(320,1) # input feature size\n",
        "  input = Input(inputShape)\n",
        "  x = Conv1D(32,(3),strides = (1),name='layer_conv1',padding='same')(input)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = MaxPooling1D((2), name='MaxPool1',padding=\"same\")(x)\n",
        "  x = Dropout(0.15)(x)\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(64,activation = 'relu',name='fc1')(x)\n",
        "  x = Dropout(0.15)(x)\n",
        "  x = Dense(2,activation = 'softmax',name='fc2')(x)\n",
        "  model = Model(inputs = input,outputs = x,name='Predict')\n",
        "  adam = Adam(learning_rate=0.001)\n",
        "  # compile the model\n",
        "  model.compile(loss='sparse_categorical_crossentropy',optimizer=adam, metrics=['accuracy'])\n",
        "  # set checkpoint and save the best model\n",
        "  mc = ModelCheckpoint('best_model.h5',  monitor='val_accuracy', mode='max', verbose=1, save_best_only=True, save_weights_only=False)\n",
        "  # summary the callbacks_list\n",
        "  callbacks_list = [mc]\n",
        "  model_history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=200,callbacks=callbacks_list,batch_size = 8, verbose=1)\n",
        "  return model, model_history"
      ],
      "metadata": {
        "id": "skIE-JAlKhPx"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JBlTA9shnQE"
      },
      "source": [
        "### model training and evaluation at test dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model & best model checkpoint will be save as 'best_model.h5'\n",
        "model, model_history = ESM_CNN(X_train, y_train, X_valid, y_valid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "LnelssIIAmWx",
        "outputId": "944c27d9-ba96-4591-933a-066d85d9c42d"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "42/52 [=======================>......] - ETA: 0s - loss: 0.8139 - accuracy: 0.6429\n",
            "Epoch 1: val_accuracy improved from -inf to 0.50485, saving model to best_model.h5\n",
            "52/52 [==============================] - 2s 9ms/step - loss: 0.7907 - accuracy: 0.6235 - val_loss: 0.6856 - val_accuracy: 0.5049\n",
            "Epoch 2/200\n",
            "40/52 [======================>.......] - ETA: 0s - loss: 0.4674 - accuracy: 0.7437"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2: val_accuracy improved from 0.50485 to 0.84466, saving model to best_model.h5\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.4920 - accuracy: 0.7359 - val_loss: 0.6623 - val_accuracy: 0.8447\n",
            "Epoch 3/200\n",
            "41/52 [======================>.......] - ETA: 0s - loss: 0.4760 - accuracy: 0.7683\n",
            "Epoch 3: val_accuracy did not improve from 0.84466\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.4717 - accuracy: 0.7726 - val_loss: 0.6412 - val_accuracy: 0.8058\n",
            "Epoch 4/200\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.4562 - accuracy: 0.7825\n",
            "Epoch 4: val_accuracy improved from 0.84466 to 0.87379, saving model to best_model.h5\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.4525 - accuracy: 0.7824 - val_loss: 0.6108 - val_accuracy: 0.8738\n",
            "Epoch 5/200\n",
            "40/52 [======================>.......] - ETA: 0s - loss: 0.3967 - accuracy: 0.8031\n",
            "Epoch 5: val_accuracy did not improve from 0.87379\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.3919 - accuracy: 0.8068 - val_loss: 0.5774 - val_accuracy: 0.8447\n",
            "Epoch 6/200\n",
            "41/52 [======================>.......] - ETA: 0s - loss: 0.3438 - accuracy: 0.8659\n",
            "Epoch 6: val_accuracy did not improve from 0.87379\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.3565 - accuracy: 0.8582 - val_loss: 0.5307 - val_accuracy: 0.8738\n",
            "Epoch 7/200\n",
            "40/52 [======================>.......] - ETA: 0s - loss: 0.3152 - accuracy: 0.8813\n",
            "Epoch 7: val_accuracy improved from 0.87379 to 0.89320, saving model to best_model.h5\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.3328 - accuracy: 0.8753 - val_loss: 0.4928 - val_accuracy: 0.8932\n",
            "Epoch 8/200\n",
            "40/52 [======================>.......] - ETA: 0s - loss: 0.3347 - accuracy: 0.8469\n",
            "Epoch 8: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.3450 - accuracy: 0.8411 - val_loss: 0.4677 - val_accuracy: 0.7767\n",
            "Epoch 9/200\n",
            "42/52 [=======================>......] - ETA: 0s - loss: 0.3149 - accuracy: 0.8482\n",
            "Epoch 9: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.3207 - accuracy: 0.8606 - val_loss: 0.4174 - val_accuracy: 0.8544\n",
            "Epoch 10/200\n",
            "40/52 [======================>.......] - ETA: 0s - loss: 0.2959 - accuracy: 0.8656\n",
            "Epoch 10: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.2787 - accuracy: 0.8778 - val_loss: 0.3494 - val_accuracy: 0.8932\n",
            "Epoch 11/200\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.2533 - accuracy: 0.8906\n",
            "Epoch 11: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.2645 - accuracy: 0.8900 - val_loss: 0.3628 - val_accuracy: 0.8641\n",
            "Epoch 12/200\n",
            "41/52 [======================>.......] - ETA: 0s - loss: 0.2278 - accuracy: 0.9085\n",
            "Epoch 12: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.2375 - accuracy: 0.8973 - val_loss: 0.3001 - val_accuracy: 0.8738\n",
            "Epoch 13/200\n",
            "41/52 [======================>.......] - ETA: 0s - loss: 0.2340 - accuracy: 0.8963\n",
            "Epoch 13: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.2201 - accuracy: 0.9046 - val_loss: 0.3169 - val_accuracy: 0.8835\n",
            "Epoch 14/200\n",
            "40/52 [======================>.......] - ETA: 0s - loss: 0.3352 - accuracy: 0.8344\n",
            "Epoch 14: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.3114 - accuracy: 0.8509 - val_loss: 0.3150 - val_accuracy: 0.8738\n",
            "Epoch 15/200\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.2248 - accuracy: 0.9020\n",
            "Epoch 15: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.2256 - accuracy: 0.9022 - val_loss: 0.3216 - val_accuracy: 0.8641\n",
            "Epoch 16/200\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.2227 - accuracy: 0.9022\n",
            "Epoch 16: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.2227 - accuracy: 0.9022 - val_loss: 0.2932 - val_accuracy: 0.8932\n",
            "Epoch 17/200\n",
            "41/52 [======================>.......] - ETA: 0s - loss: 0.1672 - accuracy: 0.9299\n",
            "Epoch 17: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.1742 - accuracy: 0.9267 - val_loss: 0.2953 - val_accuracy: 0.8835\n",
            "Epoch 18/200\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.1775 - accuracy: 0.9200\n",
            "Epoch 18: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.1782 - accuracy: 0.9193 - val_loss: 0.4135 - val_accuracy: 0.8641\n",
            "Epoch 19/200\n",
            "41/52 [======================>.......] - ETA: 0s - loss: 0.1769 - accuracy: 0.9146\n",
            "Epoch 19: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.1885 - accuracy: 0.9071 - val_loss: 0.4202 - val_accuracy: 0.8447\n",
            "Epoch 20/200\n",
            "42/52 [=======================>......] - ETA: 0s - loss: 0.1674 - accuracy: 0.9375\n",
            "Epoch 20: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.1641 - accuracy: 0.9413 - val_loss: 0.4298 - val_accuracy: 0.8641\n",
            "Epoch 21/200\n",
            "41/52 [======================>.......] - ETA: 0s - loss: 0.3025 - accuracy: 0.8872\n",
            "Epoch 21: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.2851 - accuracy: 0.8949 - val_loss: 0.3379 - val_accuracy: 0.8932\n",
            "Epoch 22/200\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.2231 - accuracy: 0.8946\n",
            "Epoch 22: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.2226 - accuracy: 0.8949 - val_loss: 0.3460 - val_accuracy: 0.8932\n",
            "Epoch 23/200\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.2046 - accuracy: 0.9044\n",
            "Epoch 23: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.2041 - accuracy: 0.9046 - val_loss: 0.3701 - val_accuracy: 0.8350\n",
            "Epoch 24/200\n",
            "43/52 [=======================>......] - ETA: 0s - loss: 0.1746 - accuracy: 0.9360\n",
            "Epoch 24: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.1647 - accuracy: 0.9389 - val_loss: 0.3271 - val_accuracy: 0.8641\n",
            "Epoch 25/200\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.1463 - accuracy: 0.9362\n",
            "Epoch 25: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.1464 - accuracy: 0.9340 - val_loss: 0.3782 - val_accuracy: 0.8544\n",
            "Epoch 26/200\n",
            "40/52 [======================>.......] - ETA: 0s - loss: 0.1425 - accuracy: 0.9344\n",
            "Epoch 26: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.1413 - accuracy: 0.9364 - val_loss: 0.3627 - val_accuracy: 0.8932\n",
            "Epoch 27/200\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.1318 - accuracy: 0.9560\n",
            "Epoch 27: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.1318 - accuracy: 0.9560 - val_loss: 0.3468 - val_accuracy: 0.8641\n",
            "Epoch 28/200\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.1360 - accuracy: 0.9461\n",
            "Epoch 28: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 7ms/step - loss: 0.1376 - accuracy: 0.9438 - val_loss: 0.3646 - val_accuracy: 0.8641\n",
            "Epoch 29/200\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.1336 - accuracy: 0.9464\n",
            "Epoch 29: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 7ms/step - loss: 0.1348 - accuracy: 0.9438 - val_loss: 0.4306 - val_accuracy: 0.8641\n",
            "Epoch 30/200\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.1191 - accuracy: 0.9412\n",
            "Epoch 30: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 7ms/step - loss: 0.1188 - accuracy: 0.9413 - val_loss: 0.4767 - val_accuracy: 0.8641\n",
            "Epoch 31/200\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.1004 - accuracy: 0.9521\n",
            "Epoch 31: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0969 - accuracy: 0.9560 - val_loss: 0.4842 - val_accuracy: 0.8738\n",
            "Epoch 32/200\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.1218 - accuracy: 0.9388\n",
            "Epoch 32: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 7ms/step - loss: 0.1176 - accuracy: 0.9413 - val_loss: 0.5273 - val_accuracy: 0.8447\n",
            "Epoch 33/200\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.1388 - accuracy: 0.9415\n",
            "Epoch 33: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 7ms/step - loss: 0.1374 - accuracy: 0.9438 - val_loss: 0.4110 - val_accuracy: 0.8447\n",
            "Epoch 34/200\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.1421 - accuracy: 0.9315\n",
            "Epoch 34: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.1421 - accuracy: 0.9315 - val_loss: 0.5111 - val_accuracy: 0.8544\n",
            "Epoch 35/200\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0865 - accuracy: 0.9609\n",
            "Epoch 35: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.0850 - accuracy: 0.9609 - val_loss: 0.5025 - val_accuracy: 0.8835\n",
            "Epoch 36/200\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.1246 - accuracy: 0.9375\n",
            "Epoch 36: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.1308 - accuracy: 0.9364 - val_loss: 0.5502 - val_accuracy: 0.8641\n",
            "Epoch 37/200\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.1096 - accuracy: 0.9550\n",
            "Epoch 37: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.1087 - accuracy: 0.9560 - val_loss: 0.5803 - val_accuracy: 0.8835\n",
            "Epoch 38/200\n",
            "40/52 [======================>.......] - ETA: 0s - loss: 0.0931 - accuracy: 0.9469\n",
            "Epoch 38: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.0987 - accuracy: 0.9389 - val_loss: 0.4501 - val_accuracy: 0.8544\n",
            "Epoch 39/200\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0858 - accuracy: 0.9658\n",
            "Epoch 39: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.0858 - accuracy: 0.9658 - val_loss: 0.5169 - val_accuracy: 0.8835\n",
            "Epoch 40/200\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.1065 - accuracy: 0.9364\n",
            "Epoch 40: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.1065 - accuracy: 0.9364 - val_loss: 0.5166 - val_accuracy: 0.8738\n",
            "Epoch 41/200\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0779 - accuracy: 0.9583\n",
            "Epoch 41: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0777 - accuracy: 0.9584 - val_loss: 0.5633 - val_accuracy: 0.8641\n",
            "Epoch 42/200\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0638 - accuracy: 0.9658\n",
            "Epoch 42: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0638 - accuracy: 0.9658 - val_loss: 0.4704 - val_accuracy: 0.8738\n",
            "Epoch 43/200\n",
            "46/52 [=========================>....] - ETA: 0s - loss: 0.0853 - accuracy: 0.9592\n",
            "Epoch 43: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0799 - accuracy: 0.9633 - val_loss: 0.4924 - val_accuracy: 0.8738\n",
            "Epoch 44/200\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0960 - accuracy: 0.9548\n",
            "Epoch 44: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0939 - accuracy: 0.9560 - val_loss: 0.6765 - val_accuracy: 0.8835\n",
            "Epoch 45/200\n",
            "41/52 [======================>.......] - ETA: 0s - loss: 0.0540 - accuracy: 0.9848\n",
            "Epoch 45: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.0621 - accuracy: 0.9756 - val_loss: 0.5869 - val_accuracy: 0.8835\n",
            "Epoch 46/200\n",
            "41/52 [======================>.......] - ETA: 0s - loss: 0.0686 - accuracy: 0.9512\n",
            "Epoch 46: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0727 - accuracy: 0.9511 - val_loss: 0.5871 - val_accuracy: 0.8544\n",
            "Epoch 47/200\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0579 - accuracy: 0.9731\n",
            "Epoch 47: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0579 - accuracy: 0.9731 - val_loss: 0.6091 - val_accuracy: 0.8544\n",
            "Epoch 48/200\n",
            "40/52 [======================>.......] - ETA: 0s - loss: 0.0541 - accuracy: 0.9625\n",
            "Epoch 48: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0542 - accuracy: 0.9633 - val_loss: 0.5875 - val_accuracy: 0.8641\n",
            "Epoch 49/200\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0585 - accuracy: 0.9707\n",
            "Epoch 49: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0623 - accuracy: 0.9682 - val_loss: 0.5591 - val_accuracy: 0.8835\n",
            "Epoch 50/200\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0610 - accuracy: 0.9682\n",
            "Epoch 50: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0610 - accuracy: 0.9682 - val_loss: 0.5918 - val_accuracy: 0.8641\n",
            "Epoch 51/200\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0834 - accuracy: 0.9550\n",
            "Epoch 51: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.0829 - accuracy: 0.9560 - val_loss: 0.5766 - val_accuracy: 0.8447\n",
            "Epoch 52/200\n",
            "40/52 [======================>.......] - ETA: 0s - loss: 0.0751 - accuracy: 0.9625\n",
            "Epoch 52: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.0715 - accuracy: 0.9633 - val_loss: 0.6710 - val_accuracy: 0.8544\n",
            "Epoch 53/200\n",
            "40/52 [======================>.......] - ETA: 0s - loss: 0.0436 - accuracy: 0.9719\n",
            "Epoch 53: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0572 - accuracy: 0.9658 - val_loss: 0.6825 - val_accuracy: 0.8544\n",
            "Epoch 54/200\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0735 - accuracy: 0.9559\n",
            "Epoch 54: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0734 - accuracy: 0.9560 - val_loss: 0.5819 - val_accuracy: 0.8835\n",
            "Epoch 55/200\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0657 - accuracy: 0.9633\n",
            "Epoch 55: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0657 - accuracy: 0.9633 - val_loss: 0.6000 - val_accuracy: 0.8738\n",
            "Epoch 56/200\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0685 - accuracy: 0.9700\n",
            "Epoch 56: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.0690 - accuracy: 0.9682 - val_loss: 0.7062 - val_accuracy: 0.8738\n",
            "Epoch 57/200\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0763 - accuracy: 0.9541\n",
            "Epoch 57: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.0749 - accuracy: 0.9560 - val_loss: 0.6083 - val_accuracy: 0.8641\n",
            "Epoch 58/200\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0793 - accuracy: 0.9531\n",
            "Epoch 58: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0768 - accuracy: 0.9560 - val_loss: 0.7851 - val_accuracy: 0.8350\n",
            "Epoch 59/200\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.1041 - accuracy: 0.9525\n",
            "Epoch 59: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.1026 - accuracy: 0.9535 - val_loss: 0.4884 - val_accuracy: 0.8738\n",
            "Epoch 60/200\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0638 - accuracy: 0.9700\n",
            "Epoch 60: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0642 - accuracy: 0.9682 - val_loss: 0.6152 - val_accuracy: 0.8738\n",
            "Epoch 61/200\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0645 - accuracy: 0.9681\n",
            "Epoch 61: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0644 - accuracy: 0.9682 - val_loss: 0.5287 - val_accuracy: 0.8738\n",
            "Epoch 62/200\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0717 - accuracy: 0.9707\n",
            "Epoch 62: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0753 - accuracy: 0.9658 - val_loss: 0.6162 - val_accuracy: 0.8447\n",
            "Epoch 63/200\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0654 - accuracy: 0.9694\n",
            "Epoch 63: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0661 - accuracy: 0.9682 - val_loss: 0.5510 - val_accuracy: 0.8641\n",
            "Epoch 64/200\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0461 - accuracy: 0.9775\n",
            "Epoch 64: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0470 - accuracy: 0.9780 - val_loss: 0.6498 - val_accuracy: 0.8447\n",
            "Epoch 65/200\n",
            "45/52 [========================>.....] - ETA: 0s - loss: 0.0615 - accuracy: 0.9611\n",
            "Epoch 65: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0688 - accuracy: 0.9584 - val_loss: 0.5426 - val_accuracy: 0.8447\n",
            "Epoch 66/200\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0634 - accuracy: 0.9792\n",
            "Epoch 66: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0643 - accuracy: 0.9780 - val_loss: 0.7494 - val_accuracy: 0.8738\n",
            "Epoch 67/200\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0526 - accuracy: 0.9707\n",
            "Epoch 67: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 7ms/step - loss: 0.0533 - accuracy: 0.9707 - val_loss: 0.6758 - val_accuracy: 0.8641\n",
            "Epoch 68/200\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0637 - accuracy: 0.9583\n",
            "Epoch 68: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0665 - accuracy: 0.9584 - val_loss: 0.5201 - val_accuracy: 0.8350\n",
            "Epoch 69/200\n",
            "46/52 [=========================>....] - ETA: 0s - loss: 0.0536 - accuracy: 0.9837\n",
            "Epoch 69: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 7ms/step - loss: 0.0492 - accuracy: 0.9853 - val_loss: 0.7832 - val_accuracy: 0.8738\n",
            "Epoch 70/200\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0416 - accuracy: 0.9840\n",
            "Epoch 70: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0422 - accuracy: 0.9829 - val_loss: 0.6903 - val_accuracy: 0.8738\n",
            "Epoch 71/200\n",
            "46/52 [=========================>....] - ETA: 0s - loss: 0.0548 - accuracy: 0.9728\n",
            "Epoch 71: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0569 - accuracy: 0.9707 - val_loss: 0.9095 - val_accuracy: 0.8835\n",
            "Epoch 72/200\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0838 - accuracy: 0.9633\n",
            "Epoch 72: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0838 - accuracy: 0.9633 - val_loss: 0.7980 - val_accuracy: 0.8738\n",
            "Epoch 73/200\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0396 - accuracy: 0.9756\n",
            "Epoch 73: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.0396 - accuracy: 0.9756 - val_loss: 0.8464 - val_accuracy: 0.8738\n",
            "Epoch 74/200\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0590 - accuracy: 0.9731\n",
            "Epoch 74: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0590 - accuracy: 0.9731 - val_loss: 0.6459 - val_accuracy: 0.8738\n",
            "Epoch 75/200\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0440 - accuracy: 0.9800\n",
            "Epoch 75: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.0430 - accuracy: 0.9804 - val_loss: 0.7763 - val_accuracy: 0.8641\n",
            "Epoch 76/200\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0636 - accuracy: 0.9657\n",
            "Epoch 76: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0634 - accuracy: 0.9658 - val_loss: 0.6781 - val_accuracy: 0.8350\n",
            "Epoch 77/200\n",
            "40/52 [======================>.......] - ETA: 0s - loss: 0.0591 - accuracy: 0.9594\n",
            "Epoch 77: val_accuracy did not improve from 0.89320\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.0747 - accuracy: 0.9560 - val_loss: 0.5174 - val_accuracy: 0.8641\n",
            "Epoch 78/200\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0770 - accuracy: 0.9633\n",
            "Epoch 78: val_accuracy improved from 0.89320 to 0.90291, saving model to best_model.h5\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0770 - accuracy: 0.9633 - val_loss: 0.5970 - val_accuracy: 0.9029\n",
            "Epoch 79/200\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0562 - accuracy: 0.9694\n",
            "Epoch 79: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.0603 - accuracy: 0.9658 - val_loss: 0.6423 - val_accuracy: 0.8835\n",
            "Epoch 80/200\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0571 - accuracy: 0.9730\n",
            "Epoch 80: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.0570 - accuracy: 0.9731 - val_loss: 0.6195 - val_accuracy: 0.8738\n",
            "Epoch 81/200\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0620 - accuracy: 0.9650\n",
            "Epoch 81: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0610 - accuracy: 0.9658 - val_loss: 0.8921 - val_accuracy: 0.8641\n",
            "Epoch 82/200\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0519 - accuracy: 0.9755\n",
            "Epoch 82: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0518 - accuracy: 0.9756 - val_loss: 0.7295 - val_accuracy: 0.8835\n",
            "Epoch 83/200\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0383 - accuracy: 0.9796\n",
            "Epoch 83: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 7ms/step - loss: 0.0368 - accuracy: 0.9804 - val_loss: 0.7815 - val_accuracy: 0.8835\n",
            "Epoch 84/200\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0551 - accuracy: 0.9657\n",
            "Epoch 84: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0550 - accuracy: 0.9658 - val_loss: 0.7353 - val_accuracy: 0.8738\n",
            "Epoch 85/200\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0843 - accuracy: 0.9654\n",
            "Epoch 85: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0958 - accuracy: 0.9658 - val_loss: 0.8242 - val_accuracy: 0.8738\n",
            "Epoch 86/200\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0655 - accuracy: 0.9633\n",
            "Epoch 86: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0655 - accuracy: 0.9633 - val_loss: 0.5519 - val_accuracy: 0.8641\n",
            "Epoch 87/200\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0646 - accuracy: 0.9745\n",
            "Epoch 87: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0620 - accuracy: 0.9756 - val_loss: 0.8253 - val_accuracy: 0.8544\n",
            "Epoch 88/200\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0347 - accuracy: 0.9828\n",
            "Epoch 88: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0346 - accuracy: 0.9829 - val_loss: 0.8274 - val_accuracy: 0.8738\n",
            "Epoch 89/200\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0428 - accuracy: 0.9779\n",
            "Epoch 89: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0427 - accuracy: 0.9780 - val_loss: 0.7713 - val_accuracy: 0.8544\n",
            "Epoch 90/200\n",
            "46/52 [=========================>....] - ETA: 0s - loss: 0.0635 - accuracy: 0.9728\n",
            "Epoch 90: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0573 - accuracy: 0.9756 - val_loss: 0.8643 - val_accuracy: 0.8641\n",
            "Epoch 91/200\n",
            "41/52 [======================>.......] - ETA: 0s - loss: 0.0329 - accuracy: 0.9817\n",
            "Epoch 91: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0424 - accuracy: 0.9756 - val_loss: 0.8186 - val_accuracy: 0.8641\n",
            "Epoch 92/200\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0429 - accuracy: 0.9780\n",
            "Epoch 92: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0429 - accuracy: 0.9780 - val_loss: 0.9469 - val_accuracy: 0.8641\n",
            "Epoch 93/200\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0709 - accuracy: 0.9632\n",
            "Epoch 93: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.0707 - accuracy: 0.9633 - val_loss: 0.8593 - val_accuracy: 0.8641\n",
            "Epoch 94/200\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0695 - accuracy: 0.9658\n",
            "Epoch 94: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.0695 - accuracy: 0.9658 - val_loss: 0.7850 - val_accuracy: 0.8447\n",
            "Epoch 95/200\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0502 - accuracy: 0.9731\n",
            "Epoch 95: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.0502 - accuracy: 0.9731 - val_loss: 0.7891 - val_accuracy: 0.8738\n",
            "Epoch 96/200\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0498 - accuracy: 0.9707\n",
            "Epoch 96: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0498 - accuracy: 0.9707 - val_loss: 0.7052 - val_accuracy: 0.8738\n",
            "Epoch 97/200\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0568 - accuracy: 0.9682\n",
            "Epoch 97: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0568 - accuracy: 0.9682 - val_loss: 0.8678 - val_accuracy: 0.8835\n",
            "Epoch 98/200\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0533 - accuracy: 0.9730\n",
            "Epoch 98: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0532 - accuracy: 0.9731 - val_loss: 0.5918 - val_accuracy: 0.8835\n",
            "Epoch 99/200\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0717 - accuracy: 0.9625\n",
            "Epoch 99: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0701 - accuracy: 0.9633 - val_loss: 0.6763 - val_accuracy: 0.8544\n",
            "Epoch 100/200\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0487 - accuracy: 0.9755\n",
            "Epoch 100: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 10ms/step - loss: 0.0486 - accuracy: 0.9756 - val_loss: 0.9855 - val_accuracy: 0.8544\n",
            "Epoch 101/200\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0543 - accuracy: 0.9682\n",
            "Epoch 101: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0543 - accuracy: 0.9682 - val_loss: 0.8228 - val_accuracy: 0.8738\n",
            "Epoch 102/200\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0746 - accuracy: 0.9707\n",
            "Epoch 102: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0746 - accuracy: 0.9707 - val_loss: 1.0534 - val_accuracy: 0.8447\n",
            "Epoch 103/200\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0499 - accuracy: 0.9681\n",
            "Epoch 103: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 1s 11ms/step - loss: 0.0508 - accuracy: 0.9682 - val_loss: 0.8633 - val_accuracy: 0.8544\n",
            "Epoch 104/200\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0295 - accuracy: 0.9804\n",
            "Epoch 104: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 1s 10ms/step - loss: 0.0294 - accuracy: 0.9804 - val_loss: 1.0467 - val_accuracy: 0.8544\n",
            "Epoch 105/200\n",
            "45/52 [========================>.....] - ETA: 0s - loss: 0.0656 - accuracy: 0.9667\n",
            "Epoch 105: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0611 - accuracy: 0.9707 - val_loss: 0.8196 - val_accuracy: 0.8738\n",
            "Epoch 106/200\n",
            "46/52 [=========================>....] - ETA: 0s - loss: 0.0464 - accuracy: 0.9728\n",
            "Epoch 106: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 7ms/step - loss: 0.0441 - accuracy: 0.9731 - val_loss: 0.8499 - val_accuracy: 0.8738\n",
            "Epoch 107/200\n",
            "44/52 [========================>.....] - ETA: 0s - loss: 0.0709 - accuracy: 0.9716\n",
            "Epoch 107: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0637 - accuracy: 0.9756 - val_loss: 0.9910 - val_accuracy: 0.8738\n",
            "Epoch 108/200\n",
            "40/52 [======================>.......] - ETA: 0s - loss: 0.0572 - accuracy: 0.9750\n",
            "Epoch 108: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 7ms/step - loss: 0.0516 - accuracy: 0.9804 - val_loss: 0.8552 - val_accuracy: 0.8738\n",
            "Epoch 109/200\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0609 - accuracy: 0.9804\n",
            "Epoch 109: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0609 - accuracy: 0.9804 - val_loss: 0.9913 - val_accuracy: 0.8641\n",
            "Epoch 110/200\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0501 - accuracy: 0.9719\n",
            "Epoch 110: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0481 - accuracy: 0.9731 - val_loss: 0.9295 - val_accuracy: 0.8447\n",
            "Epoch 111/200\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0253 - accuracy: 0.9875\n",
            "Epoch 111: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.0247 - accuracy: 0.9878 - val_loss: 1.1114 - val_accuracy: 0.8738\n",
            "Epoch 112/200\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0517 - accuracy: 0.9658\n",
            "Epoch 112: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0517 - accuracy: 0.9658 - val_loss: 0.9923 - val_accuracy: 0.8738\n",
            "Epoch 113/200\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0375 - accuracy: 0.9840\n",
            "Epoch 113: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0355 - accuracy: 0.9853 - val_loss: 0.7959 - val_accuracy: 0.8447\n",
            "Epoch 114/200\n",
            "45/52 [========================>.....] - ETA: 0s - loss: 0.0591 - accuracy: 0.9611\n",
            "Epoch 114: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0581 - accuracy: 0.9609 - val_loss: 0.7479 - val_accuracy: 0.8738\n",
            "Epoch 115/200\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0510 - accuracy: 0.9756\n",
            "Epoch 115: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0510 - accuracy: 0.9756 - val_loss: 0.9148 - val_accuracy: 0.8544\n",
            "Epoch 116/200\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0524 - accuracy: 0.9700\n",
            "Epoch 116: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0536 - accuracy: 0.9707 - val_loss: 0.8641 - val_accuracy: 0.8738\n",
            "Epoch 117/200\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0631 - accuracy: 0.9719\n",
            "Epoch 117: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0645 - accuracy: 0.9682 - val_loss: 0.8707 - val_accuracy: 0.8641\n",
            "Epoch 118/200\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0644 - accuracy: 0.9635\n",
            "Epoch 118: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0616 - accuracy: 0.9658 - val_loss: 0.6973 - val_accuracy: 0.8738\n",
            "Epoch 119/200\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0334 - accuracy: 0.9775\n",
            "Epoch 119: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 1s 13ms/step - loss: 0.0327 - accuracy: 0.9780 - val_loss: 0.8179 - val_accuracy: 0.8835\n",
            "Epoch 120/200\n",
            "43/52 [=======================>......] - ETA: 0s - loss: 0.0394 - accuracy: 0.9767\n",
            "Epoch 120: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0402 - accuracy: 0.9804 - val_loss: 0.8155 - val_accuracy: 0.8738\n",
            "Epoch 121/200\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0321 - accuracy: 0.9844\n",
            "Epoch 121: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.0369 - accuracy: 0.9829 - val_loss: 0.8921 - val_accuracy: 0.8738\n",
            "Epoch 122/200\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0341 - accuracy: 0.9814\n",
            "Epoch 122: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0408 - accuracy: 0.9756 - val_loss: 1.0834 - val_accuracy: 0.8738\n",
            "Epoch 123/200\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0672 - accuracy: 0.9675\n",
            "Epoch 123: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.0677 - accuracy: 0.9658 - val_loss: 0.8918 - val_accuracy: 0.8738\n",
            "Epoch 124/200\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0649 - accuracy: 0.9661\n",
            "Epoch 124: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0670 - accuracy: 0.9609 - val_loss: 0.7766 - val_accuracy: 0.8544\n",
            "Epoch 125/200\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0336 - accuracy: 0.9766\n",
            "Epoch 125: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.0331 - accuracy: 0.9780 - val_loss: 0.9186 - val_accuracy: 0.8641\n",
            "Epoch 126/200\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0289 - accuracy: 0.9853\n",
            "Epoch 126: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0289 - accuracy: 0.9853 - val_loss: 0.8646 - val_accuracy: 0.8641\n",
            "Epoch 127/200\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0528 - accuracy: 0.9706\n",
            "Epoch 127: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.0526 - accuracy: 0.9707 - val_loss: 0.8283 - val_accuracy: 0.8544\n",
            "Epoch 128/200\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0388 - accuracy: 0.9750\n",
            "Epoch 128: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0380 - accuracy: 0.9756 - val_loss: 0.9350 - val_accuracy: 0.8835\n",
            "Epoch 129/200\n",
            "40/52 [======================>.......] - ETA: 0s - loss: 0.0585 - accuracy: 0.9656\n",
            "Epoch 129: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.0499 - accuracy: 0.9707 - val_loss: 0.6969 - val_accuracy: 0.8835\n",
            "Epoch 130/200\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0538 - accuracy: 0.9707\n",
            "Epoch 130: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0538 - accuracy: 0.9707 - val_loss: 0.6818 - val_accuracy: 0.8544\n",
            "Epoch 131/200\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0543 - accuracy: 0.9775\n",
            "Epoch 131: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.0537 - accuracy: 0.9780 - val_loss: 1.0451 - val_accuracy: 0.8447\n",
            "Epoch 132/200\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0377 - accuracy: 0.9725\n",
            "Epoch 132: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0369 - accuracy: 0.9731 - val_loss: 0.9884 - val_accuracy: 0.8738\n",
            "Epoch 133/200\n",
            "40/52 [======================>.......] - ETA: 0s - loss: 0.0314 - accuracy: 0.9781\n",
            "Epoch 133: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0359 - accuracy: 0.9731 - val_loss: 1.0794 - val_accuracy: 0.8738\n",
            "Epoch 134/200\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0262 - accuracy: 0.9850\n",
            "Epoch 134: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.0277 - accuracy: 0.9829 - val_loss: 1.2127 - val_accuracy: 0.8447\n",
            "Epoch 135/200\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0235 - accuracy: 0.9840\n",
            "Epoch 135: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0257 - accuracy: 0.9829 - val_loss: 1.0897 - val_accuracy: 0.8738\n",
            "Epoch 136/200\n",
            "43/52 [=======================>......] - ETA: 0s - loss: 0.0583 - accuracy: 0.9622\n",
            "Epoch 136: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 7ms/step - loss: 0.0539 - accuracy: 0.9682 - val_loss: 1.2922 - val_accuracy: 0.8544\n",
            "Epoch 137/200\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0381 - accuracy: 0.9775\n",
            "Epoch 137: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 7ms/step - loss: 0.0484 - accuracy: 0.9756 - val_loss: 1.0119 - val_accuracy: 0.8641\n",
            "Epoch 138/200\n",
            "46/52 [=========================>....] - ETA: 0s - loss: 0.0386 - accuracy: 0.9837\n",
            "Epoch 138: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0348 - accuracy: 0.9853 - val_loss: 1.0722 - val_accuracy: 0.8738\n",
            "Epoch 139/200\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0540 - accuracy: 0.9740\n",
            "Epoch 139: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0515 - accuracy: 0.9756 - val_loss: 0.6672 - val_accuracy: 0.8641\n",
            "Epoch 140/200\n",
            "46/52 [=========================>....] - ETA: 0s - loss: 0.0599 - accuracy: 0.9674\n",
            "Epoch 140: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 7ms/step - loss: 0.0543 - accuracy: 0.9707 - val_loss: 1.1122 - val_accuracy: 0.8544\n",
            "Epoch 141/200\n",
            "46/52 [=========================>....] - ETA: 0s - loss: 0.0446 - accuracy: 0.9837\n",
            "Epoch 141: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0426 - accuracy: 0.9829 - val_loss: 1.1866 - val_accuracy: 0.8641\n",
            "Epoch 142/200\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0927 - accuracy: 0.9740\n",
            "Epoch 142: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 7ms/step - loss: 0.0904 - accuracy: 0.9756 - val_loss: 0.7603 - val_accuracy: 0.8641\n",
            "Epoch 143/200\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0732 - accuracy: 0.9601\n",
            "Epoch 143: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0718 - accuracy: 0.9584 - val_loss: 0.7694 - val_accuracy: 0.8641\n",
            "Epoch 144/200\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0353 - accuracy: 0.9828\n",
            "Epoch 144: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0352 - accuracy: 0.9829 - val_loss: 0.9648 - val_accuracy: 0.8835\n",
            "Epoch 145/200\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0623 - accuracy: 0.9583\n",
            "Epoch 145: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.0622 - accuracy: 0.9584 - val_loss: 0.9805 - val_accuracy: 0.8835\n",
            "Epoch 146/200\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0645 - accuracy: 0.9675\n",
            "Epoch 146: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0633 - accuracy: 0.9682 - val_loss: 0.8616 - val_accuracy: 0.8835\n",
            "Epoch 147/200\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0537 - accuracy: 0.9719\n",
            "Epoch 147: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0517 - accuracy: 0.9731 - val_loss: 0.8524 - val_accuracy: 0.8835\n",
            "Epoch 148/200\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0678 - accuracy: 0.9740\n",
            "Epoch 148: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.0661 - accuracy: 0.9731 - val_loss: 0.9187 - val_accuracy: 0.8835\n",
            "Epoch 149/200\n",
            "46/52 [=========================>....] - ETA: 0s - loss: 0.0502 - accuracy: 0.9810\n",
            "Epoch 149: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0475 - accuracy: 0.9804 - val_loss: 0.9557 - val_accuracy: 0.8641\n",
            "Epoch 150/200\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0522 - accuracy: 0.9706\n",
            "Epoch 150: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0521 - accuracy: 0.9707 - val_loss: 0.9225 - val_accuracy: 0.8641\n",
            "Epoch 151/200\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0418 - accuracy: 0.9780\n",
            "Epoch 151: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0418 - accuracy: 0.9780 - val_loss: 0.9851 - val_accuracy: 0.8641\n",
            "Epoch 152/200\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0365 - accuracy: 0.9780\n",
            "Epoch 152: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.0365 - accuracy: 0.9780 - val_loss: 0.9220 - val_accuracy: 0.8641\n",
            "Epoch 153/200\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0510 - accuracy: 0.9694\n",
            "Epoch 153: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0508 - accuracy: 0.9682 - val_loss: 1.0312 - val_accuracy: 0.8641\n",
            "Epoch 154/200\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 0.9829\n",
            "Epoch 154: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.0309 - accuracy: 0.9829 - val_loss: 1.0201 - val_accuracy: 0.8544\n",
            "Epoch 155/200\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0538 - accuracy: 0.9714\n",
            "Epoch 155: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.0506 - accuracy: 0.9731 - val_loss: 0.6376 - val_accuracy: 0.8738\n",
            "Epoch 156/200\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0366 - accuracy: 0.9740\n",
            "Epoch 156: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0344 - accuracy: 0.9756 - val_loss: 0.8995 - val_accuracy: 0.8932\n",
            "Epoch 157/200\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0558 - accuracy: 0.9675\n",
            "Epoch 157: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.0547 - accuracy: 0.9682 - val_loss: 0.7321 - val_accuracy: 0.8835\n",
            "Epoch 158/200\n",
            "52/52 [==============================] - ETA: 0s - loss: 0.0256 - accuracy: 0.9878\n",
            "Epoch 158: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.0256 - accuracy: 0.9878 - val_loss: 0.9392 - val_accuracy: 0.8932\n",
            "Epoch 159/200\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0483 - accuracy: 0.9755\n",
            "Epoch 159: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0481 - accuracy: 0.9756 - val_loss: 0.9059 - val_accuracy: 0.8641\n",
            "Epoch 160/200\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0550 - accuracy: 0.9734\n",
            "Epoch 160: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0525 - accuracy: 0.9731 - val_loss: 0.9394 - val_accuracy: 0.8544\n",
            "Epoch 161/200\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0559 - accuracy: 0.9635\n",
            "Epoch 161: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.0526 - accuracy: 0.9658 - val_loss: 0.7499 - val_accuracy: 0.8641\n",
            "Epoch 162/200\n",
            "46/52 [=========================>....] - ETA: 0s - loss: 0.0442 - accuracy: 0.9728\n",
            "Epoch 162: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0512 - accuracy: 0.9731 - val_loss: 0.9431 - val_accuracy: 0.8544\n",
            "Epoch 163/200\n",
            "44/52 [========================>.....] - ETA: 0s - loss: 0.0311 - accuracy: 0.9801\n",
            "Epoch 163: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0304 - accuracy: 0.9804 - val_loss: 1.0045 - val_accuracy: 0.8738\n",
            "Epoch 164/200\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0418 - accuracy: 0.9766\n",
            "Epoch 164: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0435 - accuracy: 0.9731 - val_loss: 0.8536 - val_accuracy: 0.8544\n",
            "Epoch 165/200\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0469 - accuracy: 0.9707\n",
            "Epoch 165: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0462 - accuracy: 0.9731 - val_loss: 0.8058 - val_accuracy: 0.8738\n",
            "Epoch 166/200\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0376 - accuracy: 0.9792\n",
            "Epoch 166: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0405 - accuracy: 0.9780 - val_loss: 0.8354 - val_accuracy: 0.8641\n",
            "Epoch 167/200\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0345 - accuracy: 0.9796\n",
            "Epoch 167: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0330 - accuracy: 0.9804 - val_loss: 0.9666 - val_accuracy: 0.8738\n",
            "Epoch 168/200\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0350 - accuracy: 0.9792\n",
            "Epoch 168: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.0383 - accuracy: 0.9780 - val_loss: 1.0168 - val_accuracy: 0.8738\n",
            "Epoch 169/200\n",
            "46/52 [=========================>....] - ETA: 0s - loss: 0.0427 - accuracy: 0.9755\n",
            "Epoch 169: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0429 - accuracy: 0.9756 - val_loss: 1.1101 - val_accuracy: 0.8738\n",
            "Epoch 170/200\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0303 - accuracy: 0.9787\n",
            "Epoch 170: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0279 - accuracy: 0.9804 - val_loss: 1.0345 - val_accuracy: 0.8641\n",
            "Epoch 171/200\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0260 - accuracy: 0.9900\n",
            "Epoch 171: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0317 - accuracy: 0.9878 - val_loss: 1.1653 - val_accuracy: 0.8641\n",
            "Epoch 172/200\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0270 - accuracy: 0.9755\n",
            "Epoch 172: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.0269 - accuracy: 0.9756 - val_loss: 1.0326 - val_accuracy: 0.8738\n",
            "Epoch 173/200\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0387 - accuracy: 0.9694\n",
            "Epoch 173: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0391 - accuracy: 0.9682 - val_loss: 0.9778 - val_accuracy: 0.8738\n",
            "Epoch 174/200\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0226 - accuracy: 0.9853\n",
            "Epoch 174: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0226 - accuracy: 0.9853 - val_loss: 0.8628 - val_accuracy: 0.8641\n",
            "Epoch 175/200\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0333 - accuracy: 0.9821\n",
            "Epoch 175: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0320 - accuracy: 0.9829 - val_loss: 0.7386 - val_accuracy: 0.8641\n",
            "Epoch 176/200\n",
            "51/52 [============================>.] - ETA: 0s - loss: 0.0450 - accuracy: 0.9755\n",
            "Epoch 176: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 7ms/step - loss: 0.0449 - accuracy: 0.9756 - val_loss: 0.8572 - val_accuracy: 0.8835\n",
            "Epoch 177/200\n",
            "44/52 [========================>.....] - ETA: 0s - loss: 0.0263 - accuracy: 0.9858\n",
            "Epoch 177: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0316 - accuracy: 0.9829 - val_loss: 0.9279 - val_accuracy: 0.8835\n",
            "Epoch 178/200\n",
            "46/52 [=========================>....] - ETA: 0s - loss: 0.0425 - accuracy: 0.9755\n",
            "Epoch 178: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0420 - accuracy: 0.9780 - val_loss: 0.7753 - val_accuracy: 0.8738\n",
            "Epoch 179/200\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0579 - accuracy: 0.9654\n",
            "Epoch 179: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 7ms/step - loss: 0.0681 - accuracy: 0.9633 - val_loss: 0.9783 - val_accuracy: 0.8835\n",
            "Epoch 180/200\n",
            "45/52 [========================>.....] - ETA: 0s - loss: 0.0935 - accuracy: 0.9556\n",
            "Epoch 180: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0971 - accuracy: 0.9535 - val_loss: 0.6420 - val_accuracy: 0.8641\n",
            "Epoch 181/200\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0708 - accuracy: 0.9719\n",
            "Epoch 181: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0679 - accuracy: 0.9731 - val_loss: 1.1408 - val_accuracy: 0.8641\n",
            "Epoch 182/200\n",
            "44/52 [========================>.....] - ETA: 0s - loss: 0.0646 - accuracy: 0.9744\n",
            "Epoch 182: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 8ms/step - loss: 0.0658 - accuracy: 0.9756 - val_loss: 0.7454 - val_accuracy: 0.8738\n",
            "Epoch 183/200\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0353 - accuracy: 0.9787\n",
            "Epoch 183: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 9ms/step - loss: 0.0364 - accuracy: 0.9804 - val_loss: 1.1847 - val_accuracy: 0.8544\n",
            "Epoch 184/200\n",
            "44/52 [========================>.....] - ETA: 0s - loss: 0.1319 - accuracy: 0.9602\n",
            "Epoch 184: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 7ms/step - loss: 0.1302 - accuracy: 0.9584 - val_loss: 0.8850 - val_accuracy: 0.8641\n",
            "Epoch 185/200\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0898 - accuracy: 0.9439\n",
            "Epoch 185: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0922 - accuracy: 0.9389 - val_loss: 0.9081 - val_accuracy: 0.8932\n",
            "Epoch 186/200\n",
            "46/52 [=========================>....] - ETA: 0s - loss: 0.0478 - accuracy: 0.9674\n",
            "Epoch 186: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0438 - accuracy: 0.9707 - val_loss: 0.8089 - val_accuracy: 0.9029\n",
            "Epoch 187/200\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0409 - accuracy: 0.9796\n",
            "Epoch 187: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0393 - accuracy: 0.9804 - val_loss: 0.8030 - val_accuracy: 0.8738\n",
            "Epoch 188/200\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0350 - accuracy: 0.9792\n",
            "Epoch 188: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.0378 - accuracy: 0.9780 - val_loss: 0.8983 - val_accuracy: 0.8835\n",
            "Epoch 189/200\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0343 - accuracy: 0.9770\n",
            "Epoch 189: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0349 - accuracy: 0.9756 - val_loss: 0.9514 - val_accuracy: 0.8835\n",
            "Epoch 190/200\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0336 - accuracy: 0.9707\n",
            "Epoch 190: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0372 - accuracy: 0.9682 - val_loss: 1.0044 - val_accuracy: 0.8932\n",
            "Epoch 191/200\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0462 - accuracy: 0.9668\n",
            "Epoch 191: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0476 - accuracy: 0.9682 - val_loss: 0.8590 - val_accuracy: 0.8738\n",
            "Epoch 192/200\n",
            "50/52 [===========================>..] - ETA: 0s - loss: 0.0475 - accuracy: 0.9750\n",
            "Epoch 192: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.0465 - accuracy: 0.9756 - val_loss: 0.7852 - val_accuracy: 0.8835\n",
            "Epoch 193/200\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0342 - accuracy: 0.9745\n",
            "Epoch 193: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0328 - accuracy: 0.9756 - val_loss: 0.9094 - val_accuracy: 0.8738\n",
            "Epoch 194/200\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0373 - accuracy: 0.9787\n",
            "Epoch 194: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0378 - accuracy: 0.9780 - val_loss: 0.8457 - val_accuracy: 0.8835\n",
            "Epoch 195/200\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0244 - accuracy: 0.9896\n",
            "Epoch 195: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0248 - accuracy: 0.9878 - val_loss: 0.8773 - val_accuracy: 0.8932\n",
            "Epoch 196/200\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0365 - accuracy: 0.9707\n",
            "Epoch 196: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 7ms/step - loss: 0.0397 - accuracy: 0.9658 - val_loss: 1.0322 - val_accuracy: 0.8835\n",
            "Epoch 197/200\n",
            "48/52 [==========================>...] - ETA: 0s - loss: 0.0285 - accuracy: 0.9896\n",
            "Epoch 197: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0284 - accuracy: 0.9902 - val_loss: 0.9572 - val_accuracy: 0.8641\n",
            "Epoch 198/200\n",
            "49/52 [===========================>..] - ETA: 0s - loss: 0.0249 - accuracy: 0.9872\n",
            "Epoch 198: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 6ms/step - loss: 0.0257 - accuracy: 0.9853 - val_loss: 1.0178 - val_accuracy: 0.8835\n",
            "Epoch 199/200\n",
            "47/52 [==========================>...] - ETA: 0s - loss: 0.0375 - accuracy: 0.9734\n",
            "Epoch 199: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.0363 - accuracy: 0.9731 - val_loss: 1.0120 - val_accuracy: 0.8544\n",
            "Epoch 200/200\n",
            "46/52 [=========================>....] - ETA: 0s - loss: 0.0247 - accuracy: 0.9837\n",
            "Epoch 200: val_accuracy did not improve from 0.90291\n",
            "52/52 [==============================] - 0s 5ms/step - loss: 0.0259 - accuracy: 0.9829 - val_loss: 1.0384 - val_accuracy: 0.8447\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the saved best model for performance evaluation at test dataset\n",
        "saved_model = load_model('best_model.h5')\n",
        "# result collection list\n",
        "ACC_collecton = []\n",
        "BACC_collecton = []\n",
        "Sn_collecton = []\n",
        "Sp_collecton = []\n",
        "recall_collecton = []\n",
        "precision_collecton = []\n",
        "f1_collecton = []\n",
        "MCC_collecton = []\n",
        "AUC_collecton = []\n",
        "# confusion matrix\n",
        "predicted_class= []\n",
        "predicted_protability = saved_model.predict(X_test,batch_size=1)\n",
        "for i in range(predicted_protability.shape[0]):\n",
        "  index = np.where(predicted_protability[i] == np.amax(predicted_protability[i]))[0][0] # the class with higher protability was treated as the prediction class\n",
        "  predicted_class.append(index)\n",
        "\n",
        "predicted_class = np.array(predicted_class) # transformed into a numpy for performance evaluation\n",
        "y_true = y_test\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import math\n",
        "# np.ravel() return a flatten 1D array\n",
        "TN, FP, FN, TP = confusion_matrix(y_true, predicted_class).ravel() # shape [ [True-Positive, False-positive], [False-negative, True-negative] ]\n",
        "ACC = (TP+TN)/(TP+TN+FP+FN)\n",
        "ACC_collecton.append(ACC)\n",
        "Sn_collecton.append(TP/(TP+FN))\n",
        "Sp_collecton.append(TN/(TN+FP))\n",
        "recall_collecton.append(TP/(TP+FN))\n",
        "precision_collecton.append(TP/(TP+FP))\n",
        "f1_collecton.append(2*TP/(TP+FP)*TP/(TP+FN)/ (TP/(TP+FP)+TP/(TP+FN)))\n",
        "MCC = (TP*TN-FP*FN)/math.pow(((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)),0.5)\n",
        "MCC_collecton.append(MCC)\n",
        "BACC_collecton.append(0.5*TP/(TP+FN)+0.5*TN/(TN+FP))\n",
        "# print the performance in the test dataset\n",
        "from sklearn.metrics import roc_auc_score\n",
        "AUC = roc_auc_score(y_test, predicted_protability[:,1])\n",
        "AUC_collecton.append(AUC)\n",
        "print(ACC_collecton[0])\n",
        "print(BACC_collecton[0])\n",
        "print(Sn_collecton[0])\n",
        "print(Sp_collecton[0])\n",
        "print(recall_collecton[0])\n",
        "print(precision_collecton[0])\n",
        "print(f1_collecton[0])\n",
        "print(MCC_collecton[0])\n",
        "print(AUC_collecton[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yIy3I8Sv9ra0",
        "outputId": "7e7e5251-12e4-4ffa-fcbf-4fceb5fbcc86"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "128/128 [==============================] - 0s 2ms/step\n",
            "0.953125\n",
            "0.953125\n",
            "0.953125\n",
            "0.953125\n",
            "0.953125\n",
            "0.953125\n",
            "0.953125\n",
            "0.90625\n",
            "0.981689453125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### model usage at new dataset"
      ],
      "metadata": {
        "id": "QEJgbbDfBATA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
        "# new dataset loading\n",
        "dataset = pd.read_excel('bitter_new_data.xlsx',header=0, index_col = None)\n",
        "# generate embedding for seqeunces\n",
        "sequence_list = dataset['sequence']\n",
        "embeddings_results = pd.DataFrame()\n",
        "for seq in sequence_list:\n",
        "    # the setting is just following the input format setting in ESM model, [name,sequence]\n",
        "    tuple_sequence = tuple([seq,seq])\n",
        "    peptide_sequence_list = []\n",
        "    peptide_sequence_list.append(tuple_sequence) # build a summarize list variable including all the sequence information\n",
        "    # employ ESM model for converting and save the converted data in csv format\n",
        "    one_seq_embeddings = esm_embeddings(model, alphabet, peptide_sequence_list)\n",
        "    embeddings_results= pd.concat([embeddings_results,one_seq_embeddings])\n",
        "embeddings_results.to_csv('new_data_esm2_t6_8M_UR50D_unified_320_dimension.csv')\n"
      ],
      "metadata": {
        "id": "vyxGBEdbegW9"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading scaler from new dataset\n",
        "X_train_embeddings = pd.read_csv('new_data_esm2_t6_8M_UR50D_unified_320_dimension.csv',header=0, index_col = 0,delimiter=',')\n",
        "# transform the dataset into numpy format\n",
        "X_train = np.array(X_train)\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train)\n",
        "# loading the peptide embddings\n",
        "X_new_embeddings = pd.read_csv('new_data_esm2_t6_8M_UR50D_unified_320_dimension.csv',header=0, index_col = 0,delimiter=',')\n",
        "# transform the dataset into numpy format\n",
        "X_new = np.array(X_new_embeddings)\n",
        "X_new = scaler.transform(X_new) # normalize X to 0-1 range"
      ],
      "metadata": {
        "id": "8R2P5uiHfBw0"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the saved best model for performance evaluation at test dataset\n",
        "saved_model = load_model('best_model.h5')\n",
        "# result collection list\n",
        "ACC_collecton = []\n",
        "BACC_collecton = []\n",
        "Sn_collecton = []\n",
        "Sp_collecton = []\n",
        "recall_collecton = []\n",
        "precision_collecton = []\n",
        "f1_collecton = []\n",
        "MCC_collecton = []\n",
        "AUC_collecton = []\n",
        "# confusion matrix\n",
        "predicted_class= []\n",
        "predicted_protability = saved_model.predict(X_new,batch_size=1)\n",
        "for i in range(predicted_protability.shape[0]):\n",
        "  index = np.where(predicted_protability[i] == np.amax(predicted_protability[i]))[0][0]\n",
        "  predicted_class.append(index)\n",
        "predicted_class = np.array(predicted_class)\n"
      ],
      "metadata": {
        "id": "usbXeMUDAjxv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da6fee0b-d114-4f77-ec7f-7c0f8c234fb3"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6/6 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(dataset.shape[0]):\n",
        "  dataset.iloc[i,1] = predicted_class[i]"
      ],
      "metadata": {
        "id": "RMPRadFe7f6d"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.to_excel('new_data_prediction_result.xlsx')"
      ],
      "metadata": {
        "id": "B9IPthVx7oOI"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mL5bbtrBDcLG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "95NTckuFZZzm",
        "m91cA0H5w_eY",
        "RddxugbsdR1Y",
        "j7rGiqVLwSH4",
        "1RGqyaijk-Kq",
        "yvuT95J2wXrq",
        "U3Fagh9Iw83q"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}